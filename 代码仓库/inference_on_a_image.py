import os
os.environ['CUDA_VISIBLE_DEVICES'] = '-1'#å¼ºåˆ¶ä½¿ç”¨cuda
os.environ['TRANSFORMERS_OFFLINE'] = '1'#å¼ºåˆ¶ç¦»çº¿

# é¦–å…ˆå¯¼å…¥å¹¶ä¿®è¡¥bertwarper
import sys

sys.path.insert(0, '.')

# åœ¨å¯¼å…¥ä»»ä½•groundingdinoæ¨¡å—ä¹‹å‰ï¼Œæ‰“è¡¥ä¸

import matplotlib.pyplot as plt

def monkey_patch_bertwarper():
    """çŒ´å­è¡¥ä¸ä¿®å¤bertwarper"""
    import torch

    def safe_generate_masks(tokenized, special_tokens_list, tokenizer):
        """ä¿®å¤ç‰ˆçš„generate_maskså‡½æ•°"""
        input_ids = tokenized["input_ids"]
        bs, num_token = input_ids.shape

        if num_token == 0:
            # è¿”å›ç©ºçš„tensor
            return (
                torch.zeros((bs, 0), dtype=torch.bool, device=input_ids.device),
                torch.zeros((bs, 0), dtype=torch.bool, device=input_ids.device),
                torch.zeros((bs, 0, 0), dtype=torch.bool, device=input_ids.device)
            )

        # è®¡ç®—ç‰¹æ®Štoken mask
        special_tokens_mask = torch.zeros((bs, num_token), dtype=torch.bool, device=input_ids.device)
        for special_token in special_tokens_list:
            if isinstance(special_token, int):
                special_tokens_mask |= input_ids == special_token

        # è®¡ç®—æ™®é€štoken mask
        non_special_tokens_mask = ~special_tokens_mask

        # åˆ›å»ºtransfer map
        idx_to_token_id = torch.arange(num_token, device=input_ids.device)
        token_id_to_idx = idx_to_token_id.unsqueeze(0).repeat(bs, 1)

        cate_to_token_mask_list = []
        for i in range(bs):
            cate_to_token_mask_listi = []
            non_special_indices = idx_to_token_id[non_special_tokens_mask[i]]

            for idx in non_special_indices:
                cate_to_token_mask_listi.append(token_id_to_idx[i] == idx)

            # å®‰å…¨åœ°stack
            if cate_to_token_mask_listi:
                cate_to_token_mask_list.append(torch.stack(cate_to_token_mask_listi, dim=0))
            else:
                # æ·»åŠ ç©ºçš„tensor
                cate_to_token_mask_list.append(torch.zeros((0, num_token), dtype=torch.bool, device=input_ids.device))

        # å°†åˆ—è¡¨è½¬æ¢ä¸ºtensor
        transfer_map = torch.stack(cate_to_token_mask_list, dim=0)

        return special_tokens_mask, non_special_tokens_mask, transfer_map

    # å¯¼å…¥å¹¶æ›¿æ¢å‡½æ•°
    import groundingdino.models.GroundingDINO.bertwarper as bertwarper
    bertwarper.generate_masks_with_special_tokens_and_transfer_map = safe_generate_masks
    print("âœ… bertwarperè¡¥ä¸å·²åº”ç”¨")


# åº”ç”¨è¡¥ä¸
monkey_patch_bertwarper()

import numpy as np
import torch
from PIL import Image, ImageDraw, ImageFont

import groundingdino.datasets.transforms as T
from groundingdino.models import build_model
from groundingdino.util.slconfig import SLConfig
from groundingdino.util.utils import clean_state_dict, get_phrases_from_posmap

BERT_LOCAL_PATH = r"C:\Users\24344\GroundingDINO\bert-base-uncased"

if not os.path.exists(BERT_LOCAL_PATH):
    print(f"âŒ BERTè·¯å¾„ä¸å­˜åœ¨: {BERT_LOCAL_PATH}")


# æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
required_files = ['tokenizer_config.json', 'vocab.txt', 'config.json']
missing_files = []
for f in required_files:
    file_path = os.path.join(BERT_LOCAL_PATH, f)
    if not os.path.exists(file_path):
        missing_files.append(f)

def plot_boxes_to_image(image_pil, tgt):
    H, W = tgt["size"]
    boxes = tgt["boxes"]
    labels = tgt["labels"]
    assert len(boxes) == len(labels), "boxes and labels must have same length"

    draw = ImageDraw.Draw(image_pil)
    mask = Image.new("L", image_pil.size, 0)
    mask_draw = ImageDraw.Draw(mask)

    # draw boxes and masks
    for box, label in zip(boxes, labels):
        # from 0..1 to 0..W, 0..H
        box = box * torch.Tensor([W, H, W, H])
        # from xywh to xyxy
        box[:2] -= box[2:] / 2
        box[2:] += box[:2]
        # random color
        color = tuple(np.random.randint(0, 255, size=3).tolist())
        # draw
        x0, y0, x1, y1 = box
        x0, y0, x1, y1 = int(x0), int(y0), int(x1), int(y1)

        draw.rectangle([x0, y0, x1, y1], outline=color, width=6)
        # draw.text((x0, y0), str(label), fill=color)

        font = ImageFont.load_default()
        if hasattr(font, "getbbox"):
            bbox = draw.textbbox((x0, y0), str(label), font)
        else:
            w, h = draw.textsize(str(label), font)
            bbox = (x0, y0, w + x0, y0 + h)
        # bbox = draw.textbbox((x0, y0), str(label))
        draw.rectangle(bbox, fill=color)
        draw.text((x0, y0), str(label), fill="white")

        mask_draw.rectangle([x0, y0, x1, y1], fill=255, width=6)

    return image_pil, mask


def load_image(image_path):
    # load image
    image_pil = Image.open(image_path).convert("RGB")  # load image

    transform = T.Compose(
        [
            T.RandomResize([800], max_size=1333),
            T.ToTensor(),
            T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
        ]
    )
    image, _ = transform(image_pil, None)  # 3, h, w
    return image_pil, image


def load_model_with_bert(model_config_path, model_checkpoint_path):
    """ä¿®å¤BERTåŠ è½½é—®é¢˜"""
    args = SLConfig.fromfile(model_config_path)
    args.device = "cpu"

    # æ„å»ºæ¨¡å‹
    model = build_model(args)
    checkpoint = torch.load(model_checkpoint_path, map_location="cpu")
    model.load_state_dict(clean_state_dict(checkpoint["model"]), strict=False)

    # é‡è¦ï¼šæ‰‹åŠ¨è®¾ç½®BERTå‚æ•°
    print("ğŸ”„ åŠ è½½BERT tokenizer...")
    try:
        from transformers import BertTokenizer
        tokenizer = BertTokenizer.from_pretrained(
            BERT_LOCAL_PATH,
            local_files_only=True,
            do_lower_case=True
        )
        model.tokenizer = tokenizer
        print("âœ… BERT tokenizeråŠ è½½æˆåŠŸ")

        # æµ‹è¯•tokenizer
        test_text = "dog and cat"
        tokens = tokenizer(test_text, return_tensors="pt")
        print(f"æµ‹è¯•tokenization: '{test_text}' -> {tokens.input_ids.shape}")
    except Exception as e:
        print(f"âŒ BERTåŠ è½½å¤±è´¥: {e}")
        print("å°è¯•ä½¿ç”¨é»˜è®¤tokenizer...")

    # è®¾ç½®æ¨¡å‹ä¸ºè¯„ä¼°æ¨¡å¼
    model.eval()
    model.to("cpu")

    return model


def get_grounding_output(model, image, caption, box_threshold, text_threshold=None, with_logits=True, cpu_only=False, token_spans=None):
    assert text_threshold is not None or token_spans is not None, "text_threshould and token_spans should not be None at the same time!"
    caption = caption.lower()
    caption = caption.strip()
    model = model.to("cpu")
    image = image.to("cpu")
    with torch.no_grad():
        outputs = model(image[None], captions=[caption])
    logits = outputs["pred_logits"].sigmoid()[0]  # (nq, 256)
    boxes = outputs["pred_boxes"][0]  # (nq, 4)


    logits_filt = logits.cpu().clone()
    boxes_filt = boxes.cpu().clone()
    filt_mask = logits_filt.max(dim=1)[0] > box_threshold
    logits_filt = logits_filt[filt_mask]  # num_filt, 256
    boxes_filt = boxes_filt[filt_mask]  # num_filt, 4

    # è·å–çŸ­è¯­
    pred_phrases = []
    if hasattr(model, 'tokenizer') and model.tokenizer is not None:
        tokenizer = model.tokenizer
        tokenized = tokenizer(caption)
        for logit, box in zip(logits_filt, boxes_filt):
            pred_phrase = get_phrases_from_posmap(logit > text_threshold, tokenized, tokenizer)
            pred_phrases.append(pred_phrase + f"({logit.max().item():.3f})")
    else:
        # å¦‚æœæ²¡æœ‰tokenizerï¼Œä½¿ç”¨ç®€å•æ ‡ç­¾
        for i, (logit, box) in enumerate(zip(logits_filt, boxes_filt)):
            pred_phrases.append(f"obj_{i}({logit.max().item():.3f})")

    return boxes_filt, pred_phrases


# åœ¨åŸæœ‰ä»£ç ä¸­æ·»åŠ BERTWrapperä¿®å¤
from groundingdino.models.GroundingDINO.bertwarper import \
    generate_masks_with_special_tokens_and_transfer_map as original_generate_masks


def patched_generate_masks(tokenized, special_tokens_list, tokenizer):
    """ä¿®è¡¥åçš„maskç”Ÿæˆå‡½æ•°"""
    input_ids = tokenized["input_ids"]
    if not hasattr(tokenizer, 'cls_token_id'):
        tokenizer.cls_token_id = tokenizer.cls_token
        tokenizer.sep_token_id = tokenizer.sep_token

    # ç¡®ä¿special_tokens_listæœ‰æ•ˆ
    if not special_tokens_list:
        special_tokens_list = [tokenizer.cls_token_id, tokenizer.sep_token_id]

    return original_generate_masks(tokenized, special_tokens_list, tokenizer)


def get_filtered_grounding_output(model, image, caption, box_threshold, text_threshold,
                                  max_detections=10, iou_threshold=0.5):
    """è·å–è¿‡æ»¤åçš„æ£€æµ‹ç»“æœ"""
    # è·å–åŸå§‹ç»“æœ
    boxes_filt, pred_phrases = get_grounding_output(
        model, image, caption, box_threshold, text_threshold, cpu_only=True
    )

    if len(boxes_filt) == 0:
        return boxes_filt, pred_phrases

    # 1. æŒ‰ç½®ä¿¡åº¦æ’åºå¹¶é™åˆ¶æ•°é‡
    if len(boxes_filt) > max_detections:
        # æå–æ¯ä¸ªæ¡†çš„æœ€å¤§ç½®ä¿¡åº¦
        confidences = []
        for phrase in pred_phrases:
            try:
                # ä»"cat(0.856)"ä¸­æå–0.856
                conf = float(phrase.split('(')[-1].rstrip(')'))
            except:
                conf = 0.0
            confidences.append(conf)

        # æŒ‰ç½®ä¿¡åº¦æ’åº
        sorted_indices = np.argsort(confidences)[::-1]  # é™åº
        boxes_filt = boxes_filt[sorted_indices[:max_detections]]
        pred_phrases = [pred_phrases[i] for i in sorted_indices[:max_detections]]

    # 2. åº”ç”¨éæå¤§å€¼æŠ‘åˆ¶(NMS)å»é™¤é‡å æ¡†
    if len(boxes_filt) > 1 and iou_threshold < 1.0:
        # è½¬æ¢xywhåˆ°xyxy
        boxes_xyxy = torch.zeros_like(boxes_filt)
        boxes_xyxy[:, 0] = boxes_filt[:, 0] - boxes_filt[:, 2] / 2  # x1
        boxes_xyxy[:, 1] = boxes_filt[:, 1] - boxes_filt[:, 3] / 2  # y1
        boxes_xyxy[:, 2] = boxes_filt[:, 0] + boxes_filt[:, 2] / 2  # x2
        boxes_xyxy[:, 3] = boxes_filt[:, 1] + boxes_filt[:, 3] / 2  # y2

        # è®¡ç®—ç½®ä¿¡åº¦
        confidences = []
        for phrase in pred_phrases:
            try:
                conf = float(phrase.split('(')[-1].rstrip(')'))
            except:
                conf = 0.0
            confidences.append(conf)

        # åº”ç”¨NMS
        keep_indices = torch.ops.torchvision.nms(
            boxes_xyxy,
            torch.tensor(confidences),
            iou_threshold
        )

        boxes_filt = boxes_filt[keep_indices]
        pred_phrases = [pred_phrases[i] for i in keep_indices]

    return boxes_filt, pred_phrases


def plot_clean_boxes_to_image(image_pil, tgt, min_confidence=0.2):
    """ç»˜åˆ¶æ¸…ç†åçš„è¾¹ç•Œæ¡†"""
    H, W = tgt["size"]
    boxes = tgt["boxes"]
    labels = tgt["labels"]

    # è¿‡æ»¤ä½ç½®ä¿¡åº¦ç»“æœ
    filtered_boxes = []
    filtered_labels = []

    for box, label in zip(boxes, labels):
        try:
            # æå–ç½®ä¿¡åº¦
            if '(' in label and ')' in label:
                confidence = float(label.split('(')[-1].rstrip(')'))
                if confidence >= min_confidence:
                    filtered_boxes.append(box)
                    filtered_labels.append(label)
            else:
                filtered_boxes.append(box)
                filtered_labels.append(label)
        except:
            filtered_boxes.append(box)
            filtered_labels.append(label)

    if len(filtered_boxes) == 0:
        print("âš ï¸ æ‰€æœ‰æ£€æµ‹ç»“æœç½®ä¿¡åº¦éƒ½ä½äºé˜ˆå€¼")
        return image_pil, Image.new("L", image_pil.size, 0)

    print(f"ğŸ¨ ç»˜åˆ¶ {len(filtered_boxes)} ä¸ªé«˜ç½®ä¿¡åº¦æ¡†")

    image_copy = image_pil.copy()
    draw = ImageDraw.Draw(image_copy)

    # ä¸ºçŒ«å’Œç‹—é€‰æ‹©ä¸åŒé¢œè‰²
    colors = {
        'cat': (255, 0, 0),  # çº¢è‰²
        'dog': (0, 255, 0),  # ç»¿è‰²
        'animal': (0, 0, 255),  # è“è‰²
    }

    for i, (box, label) in enumerate(zip(filtered_boxes, filtered_labels)):
        # æ ¹æ®æ ‡ç­¾é€‰æ‹©é¢œè‰²
        color = None
        for key in colors:
            if key in label.lower():
                color = colors[key]
                break

        if color is None:
            color = (255, 165, 0)  # æ©™è‰²ä½œä¸ºé»˜è®¤

        # è½¬æ¢åæ ‡
        x_center, y_center, width, height = box

        if isinstance(x_center, torch.Tensor):
            x_center = x_center.item()
            y_center = y_center.item()
            width = width.item()
            height = height.item()

        x0 = int((x_center - width / 2) * W)
        y0 = int((y_center - height / 2) * H)
        x1 = int((x_center + width / 2) * W)
        y1 = int((y_center + height / 2) * H)

        # ç¡®ä¿åæ ‡åœ¨åˆç†èŒƒå›´å†…
        x0, y0 = max(0, x0), max(0, y0)
        x1, y1 = min(W - 1, x1), min(H - 1, y1)

        # åªç»˜åˆ¶è¶³å¤Ÿå¤§çš„æ¡†ï¼ˆè¿‡æ»¤æ‰å™ªç‚¹ï¼‰
        if (x1 - x0) * (y1 - y0) < 100:  # é¢ç§¯å°äº100åƒç´ çš„å¿½ç•¥
            continue

        # ç»˜åˆ¶è¾¹ç•Œæ¡†
        draw.rectangle([x0, y0, x1, y1], outline=color, width=3)

        # ç®€åŒ–æ ‡ç­¾æ˜¾ç¤º
        if '(' in label and ')' in label:
            simple_label = label.split('(')[0].strip()
            confidence = label.split('(')[-1].rstrip(')')
            display_label = f"{simple_label} ({confidence})"
        else:
            display_label = label

        # ç»˜åˆ¶æ ‡ç­¾
        try:
            font = ImageFont.truetype("arial.ttf", 16)
        except:
            font = ImageFont.load_default()

        draw.text((x0 + 5, y0 + 5), display_label, fill=color, font=font)

    return image_copy


if __name__ == "__main__":
    # âš ï¸ å…³é”®ä¿®æ”¹ï¼šé™ä½é˜ˆå€¼ï¼
    config_file = r"C:\Users\24344\GroundingDINO\groundingdino\config\GroundingDINO_SwinT_OGC.py"
    checkpoint_path = r"C:\Users\24344\GroundingDINO\weights\groundingdino_swint_ogc.pth"
    image_path = r"C:\Users\24344\GroundingDINO\.asset\cat_dog.jpeg"

    # ğŸ”¥ ä¿®æ”¹è¿™é‡Œï¼
    text_prompt = "a dog . a cat . animal"  # æ›´æ¸…æ™°çš„æç¤º
    output_dir = "outputs"
    box_threshold = 0.2  # å¤§å¹…é™ä½ï¼
    text_threshold = 0.2  # å¤§å¹…é™ä½ï¼

    os.makedirs(output_dir, exist_ok=True)

    # 1. åŠ è½½å›¾ç‰‡
    print("ğŸ“· åŠ è½½å›¾ç‰‡...")
    image_pil, image = load_image(image_path)
    print(f"å›¾ç‰‡å°ºå¯¸: {image_pil.size}")

    # 2. åŠ è½½æ¨¡å‹ï¼ˆä½¿ç”¨ä¿®å¤ç‰ˆæœ¬ï¼‰
    print("ğŸ¤– åŠ è½½æ¨¡å‹...")
    model = load_model_with_bert(config_file, checkpoint_path)

    # 3. è¿è¡Œæ¨ç†å‰æ£€æŸ¥
    print(f"\nğŸ” æ¨ç†è®¾ç½®:")
    print(f"  æ–‡æœ¬æç¤º: '{text_prompt}'")
    print(f"  æ¡†é˜ˆå€¼: {box_threshold}")
    print(f"  æ–‡æœ¬é˜ˆå€¼: {text_threshold}")

    # 4. è¿è¡Œæ£€æµ‹
    with torch.no_grad():
        outputs = model(image[None], captions=[text_prompt])

    # æŸ¥çœ‹åŸå§‹è¾“å‡º
    logits = outputs["pred_logits"].sigmoid()[0]
    boxes = outputs["pred_boxes"][0]

    print(f"\nğŸ“Š åŸå§‹æ£€æµ‹ç»“æœ:")
    print(f"  æ€»æ£€æµ‹æ•°: {logits.shape[0]}")
    print(f"  æœ€å¤§ç½®ä¿¡åº¦: {logits.max().item():.4f}")
    print(f"  å¹³å‡ç½®ä¿¡åº¦: {logits.mean().item():.4f}")
    print(f"  ç½®ä¿¡åº¦>0.05çš„æ•°é‡: {(logits.max(dim=1)[0] > 0.05).sum().item()}")
    print(f"  ç½®ä¿¡åº¦>0.03çš„æ•°é‡: {(logits.max(dim=1)[0] > 0.03).sum().item()}")

    # 5. åº”ç”¨é˜ˆå€¼ï¼ˆä½¿ç”¨æ›´ä½çš„é˜ˆå€¼ï¼‰
    filt_mask = logits.max(dim=1)[0] > box_threshold
    logits_filt = logits[filt_mask]
    boxes_filt = boxes[filt_mask]

    print(f"\nğŸ¯ è¿‡æ»¤åç»“æœ (é˜ˆå€¼={box_threshold}):")
    print(f"  ä¿ç•™æ£€æµ‹æ•°: {len(logits_filt)}")

    # 6. è·å–é¢„æµ‹çŸ­è¯­
    pred_phrases = []
    if hasattr(model, 'tokenizer') and model.tokenizer is not None:
        tokenizer = model.tokenizer
        tokenized = tokenizer(text_prompt)
        for logit, box in zip(logits_filt, boxes_filt):
            pred_phrase = get_phrases_from_posmap(
                logit > text_threshold,
                tokenized,
                tokenizer
            )
            confidence = logit.max().item()
            pred_phrases.append(f"{pred_phrase} ({confidence:.3f})")
    else:
        for i, logit in enumerate(logits_filt):
            pred_phrases.append(f"obj_{i} ({logit.max().item():.3f})")

    # 7. æ‰“å°è¯¦ç»†ç»“æœ
    if len(boxes_filt) > 0:
        print(f"\nâœ… æ£€æµ‹åˆ° {len(boxes_filt)} ä¸ªç‰©ä½“:")
        for i, (box, phrase) in enumerate(zip(boxes_filt, pred_phrases)):
            print(f"  ç‰©ä½“{i + 1}: {phrase}")
            print(f"    è¾¹ç•Œæ¡†: [{box[0]:.3f}, {box[1]:.3f}, {box[2]:.3f}, {box[3]:.3f}]")

        # 8. ç»˜å›¾
        size = image_pil.size
        pred_dict = {
            "boxes": boxes_filt,
            "size": [size[1], size[0]],
            "labels": pred_phrases,
        }

        image_with_box, _ = plot_boxes_to_image(image_pil.copy(), pred_dict)

        # ä¿å­˜ç»“æœ
        output_path = os.path.join(output_dir, "final_result.jpg")
        image_with_box.save(output_path)

        # æ˜¾ç¤º
        plt.figure(figsize=(12, 6))
        plt.subplot(1, 2, 1)
        plt.imshow(image_pil)
        plt.title("åŸå§‹å›¾ç‰‡")
        plt.axis('off')

        plt.subplot(1, 2, 2)
        plt.imshow(image_with_box)
        plt.title(f"æ£€æµ‹ç»“æœ ({len(boxes_filt)}ä¸ªç‰©ä½“)")
        plt.axis('off')
        plt.show()

        print(f"\nâœ… ç»“æœå·²ä¿å­˜åˆ°: {output_path}")
    else:
        print(f"\nâš ï¸ æ²¡æœ‰æ£€æµ‹åˆ°ç‰©ä½“ï¼å°è¯•:")
        print("  1. é™ä½box_thresholdåˆ°0.03æˆ–0.02")
        print("  2. ä½¿ç”¨æ›´è¯¦ç»†çš„æ–‡æœ¬æç¤º")
        print("  3. æ£€æŸ¥BERTæ¨¡å‹æ˜¯å¦åŠ è½½æ­£ç¡®")

        # æ˜¾ç¤ºåŸå§‹å›¾ç‰‡
        plt.imshow(image_pil)
        plt.title("åŸå§‹å›¾ç‰‡ (æœªæ£€æµ‹åˆ°ç‰©ä½“)")
        plt.axis('off')
        plt.show()
