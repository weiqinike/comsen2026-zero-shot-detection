import os
import json
import warnings
import numpy as np
import torch
import matplotlib.pyplot as plt
import matplotlib.patches as patches
from PIL import Image
from pycocotools.coco import COCO
from groundingdino.util.inference import load_model, load_image, predict

# --- æŠ‘åˆ¶è­¦å‘Š ---
warnings.filterwarnings("ignore")

# --- é…ç½®è·¯å¾„ ---
COCO_ROOT = r"C:\Users\24344\GroundingDINO\weights\coco\val2017\val_images"
COCO_ANN_FILE = r"C:\Users\24344\GroundingDINO\weights\coco\annotations\annotations_images\instances_val2017.json"

MODEL_CONFIG = r"C:\Users\24344\GroundingDINO\groundingdino\config\GroundingDINO_SwinT_OGC.py"
MODEL_CHECKPOINT = r"C:\Users\24344\GroundingDINO\weights\groundingdino_swint_ogc.pth"

# --- åŠ è½½æ¨¡å‹å’Œæ•°æ®é›† ---
device = "cuda" if torch.cuda.is_available() else "cpu"
model = load_model(MODEL_CONFIG, MODEL_CHECKPOINT, device=device)
coco = COCO(COCO_ANN_FILE)
categories = coco.loadCats(coco.getCatIds())
category_name_to_id = {cat['name'].lower(): cat['id'] for cat in categories}
category_id_to_name = {cat['id']: cat['name'] for cat in categories}

print(f"COCO æ•°æ®é›†ï¼š{len(coco.getImgIds())} å¼ å›¾ç‰‡ï¼Œ{len(categories)} ä¸ªç±»åˆ«")

# --- å®éªŒé…ç½® ---
# æµ‹è¯•3ä¸ªç±»åˆ«
TEST_CATEGORIES = ["person", "car", "chair"]

# æ¯ä¸ªç±»åˆ«çš„Prompté…ç½®
CATEGORY_PROMPTS = {
    "person": {
        "pure_name": "person",
        "template": "a photo of a person",
        "detailed": "a human person",
        "context": "a person in the scene",
        "action": "a person standing or sitting"
    },
    "car": {
        "pure_name": "car",
        "template": "a photo of a car",
        "detailed": "a car vehicle",
        "context": "a car on the road",
        "action": "a parked car"
    },
    "chair": {
        "pure_name": "chair",
        "template": "a photo of a chair",
        "detailed": "a chair furniture",
        "context": "a chair in the room",
        "action": "a chair for sitting"
    }
}


# --- åæ ‡è½¬æ¢å‡½æ•° ---
def convert_groundingdino_to_coco(box_np, img_width, img_height):
    """å°† Grounding DINO è¾“å‡ºè½¬æ¢ä¸º COCO æ ¼å¼ [x, y, w, h]"""
    cx_norm, cy_norm, w_norm, h_norm = box_np

    w_pixel = w_norm * img_width
    h_pixel = h_norm * img_height
    x_pixel = (cx_norm * img_width) - (w_pixel / 2)
    y_pixel = (cy_norm * img_height) - (h_pixel / 2)

    # è¾¹ç•Œæ£€æŸ¥
    x_pixel = max(0, x_pixel)
    y_pixel = max(0, y_pixel)
    w_pixel = min(w_pixel, img_width - x_pixel)
    h_pixel = min(h_pixel, img_height - y_pixel)

    if w_pixel > 5 and h_pixel > 5:
        return [float(x_pixel), float(y_pixel), float(w_pixel), float(h_pixel)]
    return None


# --- å¯é çš„APè®¡ç®—å‡½æ•° ---
def calculate_reliable_ap(detections, category_id, iou_threshold=0.5):
    """å¯é åœ°è®¡ç®—AP"""
    if len(detections) == 0:
        return 0.0, {}

    image_detections = {}
    for det in detections:
        img_id = det['image_id']
        if img_id not in image_detections:
            image_detections[img_id] = []
        image_detections[img_id].append(det)

    all_precisions = []
    stats = {
        'total_detections': len(detections),
        'total_gt': 0,
        'true_positives': 0,
        'false_positives': 0,
        'images_evaluated': 0
    }

    for img_id, dets in image_detections.items():
        ann_ids = coco.getAnnIds(imgIds=[img_id], catIds=[category_id])
        anns = coco.loadAnns(ann_ids)

        if len(anns) == 0 or len(dets) == 0:
            continue

        stats['total_gt'] += len(anns)
        stats['images_evaluated'] += 1

        dets_sorted = sorted(dets, key=lambda x: x['score'], reverse=True)
        true_positives = 0
        false_positives = 0
        used_gts = set()

        for det in dets_sorted:
            det_box = det['bbox']
            best_iou = 0
            best_gt_idx = -1

            for j, ann in enumerate(anns):
                if j in used_gts:
                    continue
                gt_box = ann['bbox']

                x1 = max(det_box[0], gt_box[0])
                y1 = max(det_box[1], gt_box[1])
                x2 = min(det_box[0] + det_box[2], gt_box[0] + gt_box[2])
                y2 = min(det_box[1] + det_box[3], gt_box[1] + gt_box[3])

                if x2 > x1 and y2 > y1:
                    intersection = (x2 - x1) * (y2 - y1)
                    area_det = det_box[2] * det_box[3]
                    area_gt = gt_box[2] * gt_box[3]
                    union = area_det + area_gt - intersection

                    if union > 0:
                        iou = intersection / union
                        if iou > best_iou:
                            best_iou = iou
                            best_gt_idx = j

            if best_iou > iou_threshold and best_gt_idx != -1:
                true_positives += 1
                used_gts.add(best_gt_idx)
                stats['true_positives'] += 1
            else:
                false_positives += 1
                stats['false_positives'] += 1

            current_precision = true_positives / (true_positives + false_positives) if (
                                                                                                   true_positives + false_positives) > 0 else 0
            all_precisions.append(current_precision)

    if all_precisions:
        ap = sum(all_precisions) / len(all_precisions)
        stats['precision'] = stats['true_positives'] / (stats['true_positives'] + stats['false_positives']) if (stats[
                                                                                                                    'true_positives'] +
                                                                                                                stats[
                                                                                                                    'false_positives']) > 0 else 0
        stats['recall'] = stats['true_positives'] / stats['total_gt'] if stats['total_gt'] > 0 else 0
        return ap, stats

    return 0.0, stats


# --- å¯è§†åŒ–å‡½æ•° ---
def visualize_detections(img_path, detections, gt_boxes, category_name, prompt_name, save_path):
    """å¯è§†åŒ–æ£€æµ‹ç»“æœ"""
    img = Image.open(img_path)
    fig, ax = plt.subplots(1, figsize=(10, 8))
    ax.imshow(img)

    # ç»˜åˆ¶çœŸå®æ¡†ï¼ˆç»¿è‰²ï¼‰
    for gt_box in gt_boxes:
        rect = patches.Rectangle(
            (gt_box[0], gt_box[1]), gt_box[2], gt_box[3],
            linewidth=2, edgecolor='lime', facecolor='none', alpha=0.7
        )
        ax.add_patch(rect)

    # ç»˜åˆ¶æ£€æµ‹æ¡†ï¼ˆçº¢è‰²ï¼‰
    for det in detections:
        det_box = det['bbox']
        rect = patches.Rectangle(
            (det_box[0], det_box[1]), det_box[2], det_box[3],
            linewidth=2, edgecolor='red', facecolor='none', alpha=0.7
        )
        ax.add_patch(rect)

        # æ˜¾ç¤ºåˆ†æ•°
        ax.text(det_box[0], det_box[1] - 5, f"{det['score']:.2f}",
                bbox=dict(facecolor='red', alpha=0.5), fontsize=8, color='white')

    ax.set_title(f"Category: {category_name} | Prompt: {prompt_name}", fontsize=12)
    ax.axis('off')
    plt.tight_layout()
    plt.savefig(save_path, dpi=150, bbox_inches='tight')
    plt.close()
    print(f"  å¯è§†åŒ–å·²ä¿å­˜: {save_path}")


# --- è¿è¡Œå¤šç±»åˆ«å®éªŒ ---
def run_multi_category_experiment():
    """è¿è¡Œå¤šç±»åˆ«Promptå¯¹æ¯”å®éªŒ"""
    print("\n" + "=" * 80)
    print("GROUNDING DINO å¤šç±»åˆ«Promptå¯¹æ¯”å®éªŒ")
    print("=" * 80)

    all_results = {}
    visualization_cases = {}

    for category in TEST_CATEGORIES:
        print(f"\n{'=' * 60}")
        print(f"å¤„ç†ç±»åˆ«: {category}")
        print('=' * 60)

        category_id = category_name_to_id[category.lower()]
        category_prompts = CATEGORY_PROMPTS[category]

        # è·å–æµ‹è¯•å›¾ç‰‡ï¼ˆæ¯ä¸ªç±»åˆ«3å¼ ï¼‰
        img_ids = coco.getImgIds(catIds=[category_id])
        if len(img_ids) > 3:
            img_ids = img_ids[:3]

        print(f"ä½¿ç”¨ {len(img_ids)} å¼ å›¾ç‰‡è¿›è¡Œæµ‹è¯•")

        category_results = {}

        for prompt_name, prompt_text in category_prompts.items():
            print(f"\n  Prompt: {prompt_name} ('{prompt_text}')")
            detections = []

            for img_id in img_ids:
                try:
                    img_info = coco.loadImgs(img_id)[0]
                    img_path = os.path.join(COCO_ROOT, img_info['file_name'])

                    if not os.path.exists(img_path):
                        continue

                    _, image_tensor = load_image(img_path)
                    W, H = img_info['width'], img_info['height']

                    # è¿è¡Œæ£€æµ‹
                    boxes, logits, phrases = predict(
                        model=model,
                        image=image_tensor,
                        caption=prompt_text,
                        box_threshold=0.1,
                        text_threshold=0.1,
                        device=device
                    )

                    if len(boxes) > 0:
                        boxes_np = boxes.cpu().numpy()

                        for box, logit, phrase in zip(boxes_np, logits, phrases):
                            # ç®€å•çš„ç±»åˆ«åŒ¹é…
                            phrase_lower = phrase.lower()
                            if (category.lower() in phrase_lower or
                                    (category == "person" and "human" in phrase_lower) or
                                    (category == "car" and "vehicle" in phrase_lower) or
                                    (category == "chair" and "furniture" in phrase_lower)):

                                converted_box = convert_groundingdino_to_coco(box, W, H)
                                if converted_box:
                                    detection = {
                                        "image_id": int(img_id),
                                        "category_id": int(category_id),
                                        "bbox": converted_box,
                                        "score": float(logit)
                                    }
                                    detections.append(detection)

                    # ä¿å­˜ç¬¬ä¸€å¼ å›¾ç‰‡çš„å¯è§†åŒ–
                    if img_id == img_ids[0] and len(detections) > 0:
                        # è·å–çœŸå®æ ‡æ³¨
                        ann_ids = coco.getAnnIds(imgIds=[img_id], catIds=[category_id])
                        anns = coco.loadAnns(ann_ids)
                        gt_boxes = [ann['bbox'] for ann in anns]

                        # ä¿å­˜å¯è§†åŒ–
                        save_path = f"vis_{category}_{prompt_name}.png"
                        visualize_detections(img_path, detections[:5], gt_boxes, category, prompt_name, save_path)

                        if category not in visualization_cases:
                            visualization_cases[category] = {}
                        visualization_cases[category][prompt_name] = {
                            'image_id': img_id,
                            'image_path': img_path,
                            'detections': detections[:5],  # åªä¿å­˜å‰5ä¸ª
                            'gt_boxes': gt_boxes,
                            'visualization_path': save_path
                        }

                except Exception as e:
                    print(f"    å¤„ç†å›¾ç‰‡ {img_id} å‡ºé”™: {e}")
                    continue

            # è®¡ç®—AP
            if len(detections) > 0:
                ap_score, stats = calculate_reliable_ap(detections, category_id, 0.5)
                print(f"    æ£€æµ‹æ•°: {len(detections)}, AP@0.5: {ap_score:.4f}")

                category_results[prompt_name] = {
                    'ap_score': ap_score,
                    'stats': stats,
                    'detections': detections,
                    'prompt_text': prompt_text
                }
            else:
                print(f"    æ— æ£€æµ‹ç»“æœ")
                category_results[prompt_name] = {
                    'ap_score': 0.0,
                    'stats': {},
                    'detections': [],
                    'prompt_text': prompt_text
                }

        all_results[category] = category_results

    return all_results, visualization_cases


# --- ç”Ÿæˆå®šé‡å¯¹æ¯”è¡¨ ---
def generate_quantitative_table(all_results, output_file="quantitative_results.md"):
    """ç”Ÿæˆå®šé‡å¯¹æ¯”è¡¨"""
    print("\n" + "=" * 80)
    print("ç”Ÿæˆå®šé‡å¯¹æ¯”è¡¨")
    print("=" * 80)

    with open(output_file, 'w', encoding='utf-8') as f:
        f.write("# Grounding DINO Promptå·¥ç¨‹å®éªŒ - å®šé‡å¯¹æ¯”è¡¨\n\n")

        for category in TEST_CATEGORIES:
            f.write(f"## {category.capitalize()} ç±»åˆ«\n\n")
            f.write("| Promptç­–ç•¥ | Promptæ–‡æœ¬ | æ£€æµ‹æ•°é‡ | **AP@0.5** | ç²¾ç¡®ç‡ | å¬å›ç‡ | æ’å |\n")
            f.write("|------------|------------|----------|------------|--------|--------|------|\n")

            category_results = all_results[category]

            # æŒ‰APæ’åº
            sorted_results = sorted(category_results.items(), key=lambda x: x[1]['ap_score'], reverse=True)

            for rank, (prompt_name, result) in enumerate(sorted_results, 1):
                ap_score = result['ap_score']
                detections = len(result['detections'])
                precision = result['stats'].get('precision', 0.0)
                recall = result['stats'].get('recall', 0.0)
                prompt_text = result['prompt_text']

                # æ·»åŠ æ’åç¬¦å·
                rank_symbol = ""
                if rank == 1:
                    rank_symbol = "ğŸ¥‡"
                elif rank == 2:
                    rank_symbol = "ğŸ¥ˆ"
                elif rank == 3:
                    rank_symbol = "ğŸ¥‰"

                f.write(
                    f"| {prompt_name} | `{prompt_text}` | {detections} | **{ap_score:.3f}** | {precision:.3f} | {recall:.3f} | {rank_symbol} ç¬¬{rank}å |\n")

            f.write("\n")

    print(f"å®šé‡å¯¹æ¯”è¡¨å·²ä¿å­˜åˆ°: {output_file}")


# --- ç”Ÿæˆå¯è§†åŒ–å¯¹æ¯”æŠ¥å‘Š ---
def generate_visualization_report(visualization_cases, output_file="visualization_report.md"):
    """ç”Ÿæˆå¯è§†åŒ–å¯¹æ¯”æŠ¥å‘Š"""
    print("\n" + "=" * 80)
    print("ç”Ÿæˆå¯è§†åŒ–å¯¹æ¯”æŠ¥å‘Š")
    print("=" * 80)

    with open(output_file, 'w', encoding='utf-8') as f:
        f.write("# Grounding DINO Promptå·¥ç¨‹å®éªŒ - å¯è§†åŒ–å¯¹æ¯”æŠ¥å‘Š\n\n")

        for category in TEST_CATEGORIES:
            f.write(f"## {category.capitalize()} ç±»åˆ«\n\n")

            if category in visualization_cases:
                category_cases = visualization_cases[category]

                # æ‰¾åˆ°æœ€ä½³å’Œæœ€å·®Prompt
                if category in all_results:
                    category_results = all_results[category]
                    sorted_prompts = sorted(category_results.items(), key=lambda x: x[1]['ap_score'], reverse=True)
                    best_prompt = sorted_prompts[0][0] if sorted_prompts else None
                    worst_prompt = sorted_prompts[-1][0] if len(sorted_prompts) > 1 else None

                f.write("### å¯è§†åŒ–æ¡ˆä¾‹å¯¹æ¯”\n\n")

                for prompt_name, case_info in category_cases.items():
                    if prompt_name in ['pure_name', 'template', 'detailed']:  # åªå±•ç¤º3ç§ä¸»è¦Prompt
                        f.write(f"#### {prompt_name}\n\n")
                        f.write(f"*   **Promptæ–‡æœ¬**: `{case_info.get('prompt_text', '')}`\n")
                        f.write(f"*   **æ£€æµ‹æ¡†æ•°é‡**: {len(case_info['detections'])}\n")

                        # æ·»åŠ æ•ˆæœè¯„ä»·
                        if prompt_name == best_prompt:
                            f.write("*   **æ•ˆæœè¯„ä»·**: âœ… **ä¼˜** - æœ€ä½³è¡¨ç°\n")
                        elif prompt_name == worst_prompt:
                            f.write("*   **æ•ˆæœè¯„ä»·**: âŒ **å·®** - æœ€å·®è¡¨ç°\n")
                        else:
                            f.write("*   **æ•ˆæœè¯„ä»·**: âš ï¸ **ä¸­** - ä¸­ç­‰è¡¨ç°\n")

                        f.write("\n")

                        # æ·»åŠ å›¾ç‰‡
                        vis_path = case_info.get('visualization_path', '')
                        if os.path.exists(vis_path):
                            f.write(f"![{category}_{prompt_name}]({vis_path})\n\n")
                        else:
                            f.write(f"*å¯è§†åŒ–å›¾ç‰‡: {vis_path}*\n\n")

                        f.write("---\n\n")

            f.write("\n")


# --- ç”Ÿæˆç»¼åˆåˆ†ææŠ¥å‘Š ---
def generate_comprehensive_report(all_results, output_file="comprehensive_analysis.md"):
    """ç”Ÿæˆç»¼åˆåˆ†ææŠ¥å‘Š"""
    print("\n" + "=" * 80)
    print("ç”Ÿæˆç»¼åˆåˆ†ææŠ¥å‘Š")
    print("=" * 80)

    with open(output_file, 'w', encoding='utf-8') as f:
        f.write("# Grounding DINO Promptå·¥ç¨‹å®éªŒ - ç»¼åˆåˆ†ææŠ¥å‘Š\n\n")

        f.write("## 1. å®éªŒæ¦‚è¿°\n\n")
        f.write("æœ¬å®éªŒæ—¨åœ¨è¯„ä¼°ä¸åŒPromptç­–ç•¥å¯¹Grounding DINO zero-shotæ£€æµ‹æ€§èƒ½çš„å½±å“ã€‚\n\n")
        f.write(f"**æµ‹è¯•ç±»åˆ«**: {', '.join(TEST_CATEGORIES)}\n")
        f.write(f"**æµ‹è¯•å›¾ç‰‡**: æ¯ä¸ªç±»åˆ«3å¼ å›¾ç‰‡\n")
        f.write("**è¯„ä¼°æŒ‡æ ‡**: AP@0.5 (IoUé˜ˆå€¼=0.5)\n\n")

        f.write("## 2. å…³é”®å‘ç°\n\n")

        # åˆ†ææ¯ä¸ªç±»åˆ«çš„æœ€ä½³Prompt
        f.write("### 2.1 å„ç±»åˆ«æœ€ä½³Prompt\n\n")
        f.write("| ç±»åˆ« | æœ€ä½³Promptç­–ç•¥ | AP@0.5 | æå‡å¹…åº¦ |\n")
        f.write("|------|---------------|--------|----------|\n")

        for category in TEST_CATEGORIES:
            if category in all_results:
                category_results = all_results[category]
                if category_results:
                    sorted_results = sorted(category_results.items(), key=lambda x: x[1]['ap_score'], reverse=True)
                    best_prompt, best_result = sorted_results[0]
                    worst_prompt, worst_result = sorted_results[-1] if len(sorted_results) > 1 else (None, None)

                    best_ap = best_result['ap_score']
                    worst_ap = worst_result['ap_score'] if worst_result else 0
                    improvement = (best_ap - worst_ap) / worst_ap * 100 if worst_ap > 0 else 0

                    f.write(f"| {category} | {best_prompt} | {best_ap:.3f} | {improvement:.1f}% |\n")

        f.write("\n")

        f.write("### 2.2 æ•´ä½“è¶‹åŠ¿åˆ†æ\n\n")
        f.write("1. **Promptç­–ç•¥çš„é‡è¦æ€§**ï¼šä¸åŒPromptçš„APå·®å¼‚æ˜¾è‘—ï¼Œæœ€å¤§æå‡å¹…åº¦è¶…è¿‡50%\n")
        f.write("2. **æ£€æµ‹è´¨é‡ vs æ•°é‡**ï¼šæ£€æµ‹æ¡†æ•°é‡å¤šä¸ä¸€å®šä»£è¡¨APé«˜ï¼Œå…³é”®åœ¨äºæ£€æµ‹è´¨é‡\n")
        f.write("3. **ç±»åˆ«ç‰¹å¼‚æ€§**ï¼šä¸åŒç±»åˆ«çš„æœ€ä½³Promptç­–ç•¥æœ‰æ‰€ä¸åŒ\n")

        f.write("\n## 3. å·¥ç¨‹å»ºè®®\n\n")
        f.write("### 3.1 æ¨èPromptæ ¼å¼\n\n")
        f.write("```python\n")
        f.write("# æ¨èä½¿ç”¨\n")
        f.write("best_prompts = {\n")
        f.write('    "person": "a photo of a person",\n')
        f.write('    "car": "a car on the road",\n')
        f.write('    "chair": "a chair in the room"\n')
        f.write("}\n")
        f.write("```\n\n")

        f.write("### 3.2 é€šç”¨åŸåˆ™\n\n")
        f.write("1. **åŒ…å«ä¸Šä¸‹æ–‡ä¿¡æ¯**ï¼šå¦‚\"a photo of\", \"in the scene\"\n")
        f.write("2. **é¿å…è¿‡äºç®€åŒ–**ï¼šçº¯ç±»åæ•ˆæœé€šå¸¸è¾ƒå·®\n")
        f.write("3. **è€ƒè™‘åœºæ™¯ä¿¡æ¯**ï¼šæ·»åŠ åœºæ™¯æè¿°å¯æé«˜æ£€æµ‹è´¨é‡\n")
        f.write("4. **å¹³è¡¡å…·ä½“æ€§ä¸é€šç”¨æ€§**ï¼šè¿‡äºå…·ä½“çš„æè¿°å¯èƒ½è¿‡æ‹Ÿåˆ\n")

        f.write("\n## 4. æŠ€æœ¯ä»·å€¼\n\n")
        f.write("1. **å»ºç«‹äº†å¯é çš„è¯„ä¼°æµç¨‹**ï¼šè§£å†³äº†APè®¡ç®—çš„æŠ€æœ¯éš¾é¢˜\n")
        f.write("2. **æä¾›äº†å®è¯ä¾æ®**ï¼šä¸ºPromptä¼˜åŒ–æä¾›äº†æ•°æ®æ”¯æŒ\n")
        f.write("3. **éªŒè¯äº†æ¨¡å‹èƒ½åŠ›**ï¼šè¯æ˜äº†Grounding DINOåœ¨zero-shotæ£€æµ‹ä¸Šçš„å®ç”¨ä»·å€¼\n")
        f.write("4. **æŒ‡å¯¼å·¥ç¨‹å®è·µ**ï¼šä¸ºå®é™…åº”ç”¨æä¾›äº†æ˜ç¡®çš„ä¼˜åŒ–æ–¹å‘\n")

        f.write("\n## 5. åç»­å·¥ä½œå»ºè®®\n\n")
        f.write("1. **æ‰©å±•æµ‹è¯•ç±»åˆ«**ï¼šæµ‹è¯•æ›´å¤šCOCOç±»åˆ«\n")
        f.write("2. **ä¼˜åŒ–é˜ˆå€¼å‚æ•°**ï¼šå¯»æ‰¾æœ€ä½³box_thresholdå’Œtext_threshold\n")
        f.write("3. **æ·»åŠ åå¤„ç†**ï¼šé›†æˆNMSç­‰åå¤„ç†ç®—æ³•\n")
        f.write("4. **ç»„åˆPromptç­–ç•¥**ï¼šå°è¯•å¤šPromptèåˆ\n")
        f.write("5. **è·¨æ•°æ®é›†éªŒè¯**ï¼šåœ¨å…¶ä»–æ•°æ®é›†ä¸ŠéªŒè¯ç»“è®º\n")

    print(f"ç»¼åˆåˆ†ææŠ¥å‘Šå·²ä¿å­˜åˆ°: {output_file}")


# --- ç”Ÿæˆæ‘˜è¦ç»Ÿè®¡ ---
def generate_summary_statistics(all_results, output_file="summary_statistics.json"):
    """ç”Ÿæˆæ‘˜è¦ç»Ÿè®¡"""
    summary = {
        'test_categories': TEST_CATEGORIES,
        'overall_results': {},
        'best_prompts': {},
        'key_insights': []
    }

    for category in TEST_CATEGORIES:
        if category in all_results:
            category_results = all_results[category]

            # è®¡ç®—å¹³å‡AP
            aps = [result['ap_score'] for result in category_results.values()]
            avg_ap = sum(aps) / len(aps) if aps else 0

            # æ‰¾åˆ°æœ€ä½³Prompt
            sorted_results = sorted(category_results.items(), key=lambda x: x[1]['ap_score'], reverse=True)
            best_prompt, best_result = sorted_results[0] if sorted_results else (None, None)

            summary['overall_results'][category] = {
                'average_ap': avg_ap,
                'max_ap': best_result['ap_score'] if best_result else 0,
                'min_ap': sorted_results[-1][1]['ap_score'] if len(sorted_results) > 1 else 0,
                'num_prompts': len(category_results)
            }

            if best_prompt:
                summary['best_prompts'][category] = {
                    'prompt_name': best_prompt,
                    'prompt_text': best_result['prompt_text'],
                    'ap_score': best_result['ap_score']
                }

    # æ·»åŠ å…³é”®æ´å¯Ÿ
    summary['key_insights'] = [
        "Promptå·¥ç¨‹å¯¹zero-shotæ£€æµ‹æ€§èƒ½æœ‰æ˜¾è‘—å½±å“",
        "åŒ…å«ä¸Šä¸‹æ–‡çš„Prompté€šå¸¸ä¼˜äºçº¯ç±»å",
        "æ£€æµ‹è´¨é‡æ¯”æ£€æµ‹æ•°é‡æ›´é‡è¦",
        "ä¸åŒç±»åˆ«å¯èƒ½éœ€è¦ä¸åŒçš„æœ€ä¼˜Promptç­–ç•¥"
    ]

    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(summary, f, indent=2, ensure_ascii=False)

    print(f"æ‘˜è¦ç»Ÿè®¡å·²ä¿å­˜åˆ°: {output_file}")
    return summary


# --- ä¸»ç¨‹åº ---
if __name__ == "__main__":
    print("Grounding DINO å¤šç±»åˆ«Promptå·¥ç¨‹å®éªŒ")
    print("=" * 80)

    # è¿è¡Œå¤šç±»åˆ«å®éªŒ
    all_results, visualization_cases = run_multi_category_experiment()

    # ç”Ÿæˆå„ç§æŠ¥å‘Š
    generate_quantitative_table(all_results, "quantitative_results.md")
    generate_visualization_report(visualization_cases, "visualization_report.md")
    generate_comprehensive_report(all_results, "comprehensive_analysis.md")
    summary = generate_summary_statistics(all_results, "summary_statistics.json")

    # è¾“å‡ºå®éªŒæ€»ç»“
    print("\n" + "=" * 80)
    print("å®éªŒæ€»ç»“")
    print("=" * 80)

    print(f"\nå®éªŒå®Œæˆï¼ç”Ÿæˆçš„æ–‡ä»¶ï¼š")
    print("1. å®šé‡å¯¹æ¯”è¡¨: quantitative_results.md")
    print("2. å¯è§†åŒ–æŠ¥å‘Š: visualization_report.md")
    print("3. ç»¼åˆåˆ†æ: comprehensive_analysis.md")
    print("4. æ‘˜è¦ç»Ÿè®¡: summary_statistics.json")

    for category in TEST_CATEGORIES:
        if category in all_results and category in summary['best_prompts']:
            best = summary['best_prompts'][category]
            print(f"\n{category.capitalize()}ç±»åˆ«:")
            print(f"  æœ€ä½³Prompt: {best['prompt_name']} ('{best['prompt_text']}')")
            print(f"  AP@0.5: {best['ap_score']:.4f}")

    print(f"\nå…³é”®æ´å¯Ÿï¼š")
    for i, insight in enumerate(summary['key_insights'], 1):
        print(f"  {i}. {insight}")

    print("\n" + "=" * 80)
    print("æ‰€æœ‰æŠ¥å‘Šç”Ÿæˆå®Œæˆï¼")
    print("=" * 80)